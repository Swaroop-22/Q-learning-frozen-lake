{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c73cd7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.4.0 (SDL 2.26.4, Python 3.10.0)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "#Importing Dependencies\n",
    "import os\n",
    "import numpy as np\n",
    "import gym\n",
    "import pickle\n",
    "import pygame\n",
    "import stable_baselines3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e69e80dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<TaxiEnv<Taxi-v3>>>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating Enviorment\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2fe642c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "387"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating State\n",
    "state = env.reset() #Initially reseting the enviorment to it's original state.\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "215782c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking For Obeservation\n",
    "env.observation_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3ae8b7",
   "metadata": {},
   "source": [
    "Currently We have total 500 observations available in this enviorment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c479f434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b68d1b0",
   "metadata": {},
   "source": [
    "Possible actions:\n",
    "\n",
    "Down: 0\n",
    "\n",
    "Up: 1\n",
    "\n",
    "Right: 2\n",
    "\n",
    "Left: 3\n",
    "\n",
    "Pickup: 4\n",
    "\n",
    "Drop-off: 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d22fe89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b600e4da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f156797",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-setting the enviorment\n",
    "env.env.s = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6445b625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : |\u001b[43m \u001b[0m: : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[34;1mY\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7b2076",
   "metadata": {},
   "source": [
    "The | in above figures are barriers(walls), By presetting the enviorment to 150 we have changed the pickup location(Y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39916b4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, -1, False, {'prob': 1.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14c512a",
   "metadata": {},
   "source": [
    "step() - Updates an environment with actions returning the next agent observation.\n",
    "The above code gives us the probability of reaching to that state. So the Probability = 1 means the passanger is inside the cab where as if its 0 then the passanger is not in cab. The value 50 are observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2a2463",
   "metadata": {},
   "source": [
    "# How good agent behaves at complete random approach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "987a0ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "counter = 0\n",
    "g = 0\n",
    "reward = None "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122ca6a4",
   "metadata": {},
   "source": [
    "counter is for the directions. Initially it set to 0\n",
    "\n",
    "g = goal state wether the agent was successful to drop off to correct locations. Initially it set to 0\n",
    "\n",
    "reward = None : This is boolean variable passing if it's successful in picking up location where passanger and drop at correct location then it will be True or else it will be False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1eda548f",
   "metadata": {},
   "outputs": [],
   "source": [
    "while reward != 20:\n",
    "    state, reward, done, info = env.step(env.action_space.sample())\n",
    "    counter += 1\n",
    "    g += reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1ce0e0",
   "metadata": {},
   "source": [
    "In the above while loop we are setting the highest reward 20. If it's not equal to the 20 then we'll enter in while loop.\n",
    "We are going to initialise the state, reward, done and info variables which are present in step function.\n",
    "\n",
    "The Step Function will perform all the action at each step, So Whenever the transition happens from one state to another state.\n",
    "At every state you will have reward, when you will reached from old state(St) to new state(St+1) you will get reward.\n",
    "It can be positive or negative reward.\n",
    "\n",
    "Done can be True in only 2 possible states, If it is at pick-up location and When if it drop off at the correct location, Else it will be False.\n",
    "\n",
    "Information is the current location of Taxi.\n",
    "\n",
    "Whenever the action is done  we are incrementing the counter and also the reward regardless it is +ve or -ve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c506766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved in 1425 steps with total reward of -5400\n"
     ]
    }
   ],
   "source": [
    "print(\"Solved in {} steps with total reward of {}\".format(counter,g))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351eb6b4",
   "metadata": {},
   "source": [
    "The agent Solved in 1425 steps with total reward of -5400 which is complete random.\n",
    "Now we have to minimise the steps and maximise the rewards.\n",
    "\n",
    "We're getting negative rewards instead of positive. \n",
    "\n",
    "So to get max and +ve rewards we will abandon the random approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e70c2bf",
   "metadata": {},
   "source": [
    "# Using Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bab04419",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialise the Q table\n",
    "Q = np.zeros([n_states, n_actions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d912d29d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065021bb",
   "metadata": {},
   "source": [
    "# Training agent for 1 episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9bd6fe69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining Hyperparameters\n",
    "episode = 1 #With how many episode the agent learn the enviroment.\n",
    "G = 0  #Goal State\n",
    "alpha = 0.618 #Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "582e24a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State = 434\n"
     ]
    }
   ],
   "source": [
    "for episode in range(1, episode+1):\n",
    "    done = False\n",
    "    G,reward = 0,0\n",
    "    state = env.reset()\n",
    "    firstState = state\n",
    "    print(\"Initial State = {}\".format(state))\n",
    "    while reward != 20:\n",
    "        action = np.argmax(Q[state])\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        Q[state,action] += alpha * (reward + np.max(Q[new_state]) - Q[state,action])\n",
    "        G += reward\n",
    "        state = new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7493eeab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "410"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FinalState = state\n",
    "FinalState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d3e9fc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.      ,  0.      ,  0.      ,  0.      ,  0.      ,  0.      ],\n",
       "       [ 0.      ,  0.      ,  0.      ,  0.      ,  0.      ,  0.      ],\n",
       "       [ 0.      ,  0.      ,  0.      ,  0.      ,  0.      ,  0.      ],\n",
       "       ...,\n",
       "       [ 0.      ,  0.      ,  0.      ,  0.      ,  0.      ,  0.      ],\n",
       "       [-1.236   , -0.854076, -1.236   , -1.236   , -6.18    , -6.18    ],\n",
       "       [ 0.      ,  0.      ,  0.      ,  0.      ,  0.      ,  0.      ]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8f088b",
   "metadata": {},
   "source": [
    "# Training agent for multiple episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e2a63ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode = 1000\n",
    "rewardTracker = [] #List of rewards\n",
    "G = 0\n",
    "alpha = 0.618"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3592a896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100 Total reward: 12\n",
      "Episode: 200 Total reward: 10\n",
      "Episode: 300 Total reward: 11\n",
      "Episode: 400 Total reward: 11\n",
      "Episode: 500 Total reward: 8\n",
      "Episode: 600 Total reward: 7\n",
      "Episode: 700 Total reward: 9\n",
      "Episode: 800 Total reward: 8\n",
      "Episode: 900 Total reward: 12\n",
      "Episode: 1000 Total reward: 6\n"
     ]
    }
   ],
   "source": [
    "for episode in range(1, episode+1):\n",
    "    done = False\n",
    "    G,reward = 0,0\n",
    "    state = env.reset()\n",
    "    while done != True:\n",
    "        action = np.argmax(Q[state])\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        Q[state,action] += alpha * (reward + np.max(Q[new_state]) - Q[state,action])\n",
    "        G += reward\n",
    "        state = new_state\n",
    "    if episode % 100 == 0:\n",
    "        print(\"Episode: {} Total reward: {}\".format(episode, G))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c6af22",
   "metadata": {},
   "source": [
    "# Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce18321e",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "done = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6678767d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| :\u001b[43m \u001b[0m: : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : :\u001b[43m \u001b[0m: |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : :\u001b[43m \u001b[0m|\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : :\u001b[43m \u001b[0m|\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[42mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "+---------+\n",
      "|R: | :\u001b[42m_\u001b[0m:G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | :\u001b[42m_\u001b[0m: |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : :\u001b[42m_\u001b[0m: |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[42m_\u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[42mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (Dropoff)\n"
     ]
    }
   ],
   "source": [
    "while done != True:\n",
    "    #We are taking the highest Q value in the action \n",
    "    action = np.argmax(Q[state])\n",
    "    state, reward, done, info = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aebd618",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
